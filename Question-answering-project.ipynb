{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ready-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import transformers\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-chile",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "received-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    \"\"\"\n",
    "    Loads the data from the json file into parallel lists of contexts, questions, answers, and another list to indicate\n",
    "    whether the question is answerable or not. Note that the context may report multiple times, as one context may have\n",
    "    several questions based on it.\n",
    "    \n",
    "    Input: path to the json data file\n",
    "    \n",
    "    Returns:\n",
    "        contexts(str): passage which contains the answer to the question\n",
    "        questions(str): question to be asked to the model\n",
    "        answers(dict): answer to the corresponding question along with the index at which it starts.\n",
    "        is_impossible(bool): tells whether the question is answerable or not\n",
    "    \"\"\"\n",
    "    \n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    is_impossible_list=[]\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                answerable = qa['is_impossible']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "                    is_impossible_list.append(answerable)\n",
    "                    \n",
    "    return contexts, questions, answers, is_impossible_list\n",
    "\n",
    "train_contexts, train_questions, train_answers, train_is_impossible = read_squad('squad-data/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers, val_is_impossible = read_squad('squad-data/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-maryland",
   "metadata": {},
   "source": [
    "Interestingly, the count of questions that are impossible to answer is 0 despite it being mentioned in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "honey-intention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_impossible = 0\n",
    "for i, j in zip(val_is_impossible, train_is_impossible):\n",
    "    if (i == True) or (j==True):\n",
    "        count_impossible+=1\n",
    "count_impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-residence",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-logan",
   "metadata": {},
   "source": [
    "The below function adds an end index which gives the position of the token at which the answer ends. Sometimes the answers in SQuAD are off by 1 or 2 indices, so we also adjust for that in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mathematical-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        answers(list): answers that correspond to a particular question.\n",
    "        contexts(list): context which contains the corresponding answer.\n",
    "    Output:\n",
    "        Adds an end index answer['answer_end'] which corresponds to the token at which the answer ends\n",
    "        and also adjusts indices to ensure they are correct.\n",
    "        \n",
    "    \"\"\"\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two â€“ fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-chemistry",
   "metadata": {},
   "source": [
    "### Tokenizing the input\n",
    "Now we need to tokenize our context/question pairs. Tokenizers accept parallel lists of sequences and encode them together as sequence pairs.\n",
    "The output of these encodings is of the form: [CLS] + tokenized_context_string + [SEP] tokenized_question_string + [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mediterranean-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizerFast\n",
    "tokenizer = AlbertTokenizerFast.from_pretrained('albert-base-v2')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-values",
   "metadata": {},
   "source": [
    "### Converting character start/end positions to token start/end positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "severe-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-seating",
   "metadata": {},
   "source": [
    "# Putting the preprocessed Data into a Pytorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "diagnostic-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "weekly-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_size = int(0.5 * len(val_dataset))\n",
    "val_dataset, test_dataset = val_dataset[:test_dataset_size], val_dataset[test_dataset_size + 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-virginia",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-accreditation",
   "metadata": {},
   "source": [
    "In order to define any custom pytorch model, we must do two things: define an __init__() function which contains all the parameters of the model and a forward() function which defines the flow of forward propogation using the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "incorporate-heath",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a0cb2b1b0147c8bb74a923473648c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129e1fe6446d4c0dba83b2d0caf90810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/236M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-xlarge-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class AlBertQA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = transformers.AlbertForQuestionAnswering.from_pretrained(config)\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs, return_dict=False)\n",
    "\n",
    "model = AlBertQA('albert-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "later-wagner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlBertQA(\n",
       "  (model): AlbertForQuestionAnswering(\n",
       "    (albert): AlbertModel(\n",
       "      (embeddings): AlbertEmbeddings(\n",
       "        (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (encoder): AlbertTransformer(\n",
       "        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (albert_layer_groups): ModuleList(\n",
       "          (0): AlbertLayerGroup(\n",
       "            (albert_layers): ModuleList(\n",
       "              (0): AlbertLayer(\n",
       "                (full_layer_layer_norm): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
       "                (attention): AlbertAttention(\n",
       "                  (query): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (key): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (value): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                  (output_dropout): Dropout(p=0, inplace=False)\n",
       "                  (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                  (LayerNorm): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
       "                )\n",
       "                (ffn): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                (ffn_output): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (qa_outputs): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "hearing-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-questionnaire",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "nutritional-imagination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "optim = Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_losses = [0]*7\n",
    "val_losses = [0]*7\n",
    "\n",
    "for epoch in range(7):\n",
    "    model.train() \n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_losses[epoch]= loss.item()\n",
    "    #evaluate on val set\n",
    "    model.eval()\n",
    "    val_batch = next(iter(val_dataset))\n",
    "    input_ids = val_batch['input_ids'].to(device)\n",
    "    attention_mask = val_batch['attention_mask'].to(device)\n",
    "    start_positions = val_batch['start_positions'].to(device)\n",
    "    end_positions = val_batch['end_positions'].to(device)\n",
    "    val_outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    val_loss = val_outputs[0]\n",
    "    val_losses[epoch] = val_loss.item()\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "train_losses_np = np.array(train_losses)\n",
    "val_losses_np = np.array(val_losses)\n",
    "epoch = np.array([1,2,3,4,5,6,7])\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(epoch, train_losses_np, '-b', label = 'Train losses')\n",
    "plt.plot(epoch, val_losses_np, '-r', label = 'Validation losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "light-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/AlbertQACustom'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-mills",
   "metadata": {},
   "source": [
    "# Testing the Model - F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "union-thailand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score of the model on the test set is:  84.126\n"
     ]
    }
   ],
   "source": [
    "#In order to test the performance of our model, we compute the F1 score on it\n",
    "test_dataloader = Dataloader(test_dataset, batch_size = len(test_dataset))\n",
    "output_ = model(next(iter(test_dataloader)))\n",
    "output_batch = (torch.argmax(output_[1]), torch.argmax(output_[2])) \n",
    "ground_truth = (test_data['answer_start'], test_data['answer_end'])\n",
    "import collections\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "score = compute_f1(ground_truth, output_batch)\n",
    "score = \"{:.3f}\".format(score)\n",
    "print(\"The F1 score of the model on the test set is: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-exercise",
   "metadata": {},
   "source": [
    "## Testing the Model on our own inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "floral-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 28th september 2000\n"
     ]
    }
   ],
   "source": [
    "context_1 = \"Parth Sharma is a student at DTU. He was born on 28th september 2000. \\\n",
    "His mother was born on 23rd april. He loves watching tv shows and playing table tennis.\"\n",
    "question_1 = \"When was Parth Sharma born?\"\n",
    "input_ids = tokenizer.encode(context_1, question_1)\n",
    "print('Input ids', input_ids)\n",
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "inputs = torch.tensor([input_ids])\n",
    "start_scores, end_scores = model(inputs, token_type_ids=torch.tensor([token_type_ids]))\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "print('Answer:', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "polish-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Rajouri Garden\n"
     ]
    }
   ],
   "source": [
    "context_2 = \"Nishant Soni is a student of Delhi Technological University in Bawana, and lives in Rajouri Garden. \\\n",
    "He loves going to Connaught place and eat at the restaurants there. \"\n",
    "question_2 = \"Where does Nishant Soni live?\"\n",
    "input_ids = tokenizer.encode(context_2, question_2)\n",
    "print('Input ids', input_ids)\n",
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "inputs = torch.tensor([input_ids])\n",
    "start_scores, end_scores = model(inputs, token_type_ids=torch.tensor([token_type_ids]))\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "print('Answer:', answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
